{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test env setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object sparkSqlDemo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 30|   John|\n",
      "| 28|    Tom|\n",
      "| 35|    Jim|\n",
      "| 40|  Randy|\n",
      "| 56|  Bryan|\n",
      "| 22|Shannon|\n",
      "| 33|   Rick|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "object sparkSqlDemo {\n",
    "    val sparkSession = SparkSession.builder()\n",
    "        .master(\"local[1]\")\n",
    "        .appName(\"spark session example\")\n",
    "        .getOrCreate()\n",
    "\n",
    "    def main(args: Array[String]) {\n",
    "        val input = sparkSession.read.json(\"jsonSample.json\")\n",
    "        input.createOrReplaceTempView(\"Cars1\")\n",
    "        val result = sparkSession.sql(\"select * from Cars1\")\n",
    "        result.show()\n",
    "    }\n",
    "}\n",
    "\n",
    "sparkSqlDemo.main(Array()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lending club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.internal.Logging\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class LoanType\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class LoanType (\n",
    "                      loan_amnt: Option[String],\n",
    "                      term: Option[String],\n",
    "                      int_rate: Option[String],\n",
    "                      installment: Option[String],\n",
    "                      home_ownership: Option[String],\n",
    "                      annual_inc: Option[String],\n",
    "                      DTI: Option[String],\n",
    "                      addr_state: Option[String],\n",
    "                      emp_length: Option[String],\n",
    "                      title: Option[String],\n",
    "                      has_collection: Option[Int]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readLoanData: (inputPath: String, spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.Dataset[LoanType]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def readLoanData(inputPath: String, spark: SparkSession): Dataset[LoanType] = {\n",
    "\n",
    "    import spark.implicits._\n",
    "\n",
    "    val rawData = spark.read.option(\"header\", \"true\").csv(inputPath)\n",
    "\n",
    "    printf(\"reading data from %s\".format(inputPath))\n",
    "\n",
    "    val filteredRawDf = rawData\n",
    "      .filter($\"loan_status\" =!= \"Fully Paid\") //not fully paid\n",
    "\n",
    "    val fields = List(\"loan_amnt\", \"term\", \"int_rate\", \"installment\", \"home_ownership\",\n",
    "      \"annual_inc\", \"emp_length\", \"title\", \"addr_state\", \"loan_status\", \"tot_coll_amt\").map(col)\n",
    "\n",
    "    filteredRawDf.select(fields: _*)\n",
    "      .withColumn(\"has_collection\", when($\"tot_coll_amt\" =!= \"0\", 1).otherwise(0).as(\"has_collection\"))\n",
    "      .withColumn(\"DTI\", $\"installment\"/($\"annual_inc\"/12))\n",
    "      .drop(\"loan_status\")\n",
    "      .drop(\"tot_coll_amt\")\n",
    "      .as[LoanType]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readRejectionData: (inputPath: String, spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.Dataset[LoanType]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//sjlakfjdl;sk\n",
    "def readRejectionData(inputPath: String, spark: SparkSession): Dataset[LoanType] = {\n",
    "\n",
    "    import spark.implicits._\n",
    "\n",
    "    val rawData = spark.read.option(\"header\", \"true\").csv(inputPath)\n",
    "\n",
    "    printf(\"reading data from %s\".format(inputPath))\n",
    "\n",
    "    val fields = List(\"Amount Requested\", \"Loan Title\", \"Debt-To-Income Ratio\", \"State\", \"Employment Length\").map(col)\n",
    "\n",
    "    rawData.select(fields: _*)\n",
    "      .withColumnRenamed(\"Amount Requested\", \"loan_amnt\")\n",
    "      .withColumnRenamed(\"Loan Title\", \"title\")\n",
    "      .withColumnRenamed(\"Debt-To-Income Ratio\", \"DTI\")\n",
    "      .withColumnRenamed(\"State\", \"addr_state\")\n",
    "      .withColumnRenamed(\"Employment Length\", \"emp_length\")\n",
    "      .withColumn(\"term\", lit(null: StringType))\n",
    "      .withColumn(\"int_rate\", lit(null: StringType))\n",
    "      .withColumn(\"installment\", lit(null: StringType))\n",
    "      .withColumn(\"home_ownership\", lit(null: StringType))\n",
    "      .withColumn(\"annual_inc\", lit(null: StringType))\n",
    "      .withColumn(\"has_collection\", lit(0))\n",
    "      .as[LoanType]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loanInfoAggregator: (rejectionDs: org.apache.spark.sql.Dataset[LoanType], loanDs: org.apache.spark.sql.Dataset[LoanType], spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loanInfoAggregator(rejectionDs: Dataset[LoanType], loanDs: Dataset[LoanType], spark: SparkSession): DataFrame = {\n",
    "    import spark.implicits._\n",
    "\n",
    "    val unionDs = rejectionDs.unionByName(loanDs)\n",
    "\n",
    "    val aggregatedDf = unionDs.groupBy(\"term\", \"home_ownership\", \"addr_state\", \"title\", \"emp_length\")\n",
    "      .agg(avg($\"loan_amnt\").as(\"avg_loan_amnt\"),\n",
    "        avg($\"int_rate\").as(\"avg_int_rate\"),\n",
    "        avg($\"annual_inc\").as(\"avg_annual_inc\"),\n",
    "        avg($\"DTI\").as(\"avg_DTI\"),\n",
    "        sum($\"has_collection\").as(\"sum_has_collection\"),\n",
    "        avg($\"installment\").as(\"avg_installment\")\n",
    "      )\n",
    "\n",
    "    aggregatedDf\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "writeLoanAggregatedData: (outputDataframe: org.apache.spark.sql.DataFrame, outputPath: String)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def writeLoanAggregatedData(outputDataframe: DataFrame, outputPath: String): Unit = {\n",
    "    outputDataframe.repartition(1).write.json(outputPath)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from /Users/jenny/Desktop/a/LoanStats_2019Q1.csvreading data from /Users/jenny/Desktop/a/RejectStats_2019Q1.csv"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@b8c2568\n",
       "loanInputPath = /Users/jenny/Desktop/a/LoanStats_2019Q1.csv\n",
       "rejectionInputPath = /Users/jenny/Desktop/a/RejectStats_2019Q1.csv\n",
       "outputPath = /Users/jenny/Desktop/b\n",
       "loanDs = [loan_amnt: string, term: string ... 9 more fields]\n",
       "rejectionDs = [loan_amnt: string, title: string ... 9 more fields]\n",
       "aggregatedDf = [term: string, home_ownership: string ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[term: string, home_ownership: string ... 9 more fields]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "  .master(\"local[*]\")\n",
    "  .builder()\n",
    "  .appName(\"Loan-analyze\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val loanInputPath = \"/Users/jenny/Desktop/a/LoanStats_2019Q1.csv\"\n",
    "val rejectionInputPath = \"/Users/jenny/Desktop/a/RejectStats_2019Q1.csv\"\n",
    "val outputPath = \"/Users/jenny/Desktop/b\"\n",
    "\n",
    "val loanDs = readLoanData(loanInputPath, spark)\n",
    "\n",
    "val rejectionDs = readRejectionData(rejectionInputPath, spark)\n",
    "\n",
    "val aggregatedDf = loanInfoAggregator(rejectionDs, loanDs, spark)\n",
    "\n",
    "writeLoanAggregatedData(aggregatedDf, outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# city bike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unique-user: \n",
    "\n",
    "the first time user used bike share\n",
    "\n",
    "daily updatedï¼›Unique-user size will be larger and larger once there are daily new users\n",
    "\n",
    "- Out-put:\n",
    "\n",
    "[folder] average usage duration on the day\n",
    "\n",
    "[1-day-retention folder] user_id\n",
    "\n",
    "[3-day-retention folder] user_id\n",
    "\n",
    "[7-day-retention folder] user_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bike = [bike_number: string, bike_share_for_all_trip: string ... 19 more fields]\n",
       "user = [user_id: string, bike_share_for_all_trip: string ... 19 more fields]\n",
       "userDate = [user_id: string, bike_share_for_all_trip: string ... 20 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[user_id: string, bike_share_for_all_trip: string ... 20 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bike = spark.read.json(\"/Users/jenny/Downloads/bike-bike-trips.json\")\n",
    "val user = bike.withColumnRenamed(\"bike_number\", \"user_id\").withColumnRenamed(\"start_date\", \"start_timestamp\")\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "val userDate = user.withColumn(\"start_date\", to_date($\"start_timestamp\", \"yyyy-MM-dd\"))\n",
    "userDate.write.partitionBy(\"start_date\").json(\"/Users/jenny/Desktop/bike_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.rogach:scallop_2.11:3.4.0 for download\n",
      "Obtained 2 files\n",
      "Marking org.scalatest:scalatest_2.11:3.0.1 for download\n",
      "Obtained 2 files\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.rogach scallop_2.11 3.4.0 \n",
    "%AddDeps org.scalatest scalatest_2.11 3.0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class CohortConf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// process the commond line\n",
    "import org.rogach.scallop.{ScallopConf, ScallopOption}  //load pack at the same cell\n",
    "class CohortConf(args: Seq[String]) extends ScallopConf(args) with Serializable {\n",
    "\n",
    "  val selectColumnsConfigFile: ScallopOption[String] =\n",
    "    opt[String](\"select.columns.config\",\n",
    "      descr = \"name of columns that you want to select\",\n",
    "      required = false,\n",
    "      default = Option(\"select-columns\"))\n",
    "\n",
    "  val bikeTripKey: ScallopOption[String] =\n",
    "    opt[String](\"bike.trip.key\",\n",
    "      descr = \"bike trip path key\",\n",
    "      required = false,\n",
    "      default = Option(\"bike-trips\"))\n",
    "\n",
    "  val env: ScallopOption[String] =\n",
    "    opt[String](\"env\",\n",
    "      descr = \"env name that job is running on, test, stage, prod\",\n",
    "      required = false,\n",
    "      default = Option(\"stage\")) //show environment handling\n",
    "\n",
    "  val inputBikeSharePath: ScallopOption[String] =\n",
    "    opt[String](\"input.bike.path\",\n",
    "      descr = \"input data path for bike share data\",\n",
    "      required = false,\n",
    "      default = env() match {\n",
    "        case \"test\" => Option(\"/Users/jenny/Desktop/bike_data\")\n",
    "        case \"stage\" => Option(\"/Users/jenny/Desktop/bike_data\")\n",
    "        case \"prod\" => Option(\"/Users/jenny/Desktop/bike_data\")\n",
    "        case _ => None\n",
    "          throw new Exception(s\"env error, env name can either be test, stage, prod \\ncannot be ${env()}\")\n",
    "      })\n",
    "\n",
    "  val inputMetaDataPath: ScallopOption[String] =\n",
    "    opt[String](\"input.meta.path\",\n",
    "      descr = \"input meta data parent path\",\n",
    "      required = false,\n",
    "      default = env() match {\n",
    "        case \"test\" => Option(\"/Users/jenny/Desktop/meta\")\n",
    "        case \"stage\" => Option(\"/Users/jenny/Desktop/meta\")\n",
    "        case \"prod\" => Option(\"/Users/jenny/Desktop/meta\")\n",
    "        case _ => None\n",
    "          throw new Exception(s\"env error, env name can either be test, stage, prod \\ncannot be ${env()}\")\n",
    "      })\n",
    "\n",
    "  val outputDataPath: ScallopOption[String] =\n",
    "    opt[String](\"output.data.path\",\n",
    "      descr = \"output data parent path\",\n",
    "      required = false,\n",
    "      default = env() match {\n",
    "        case \"test\" => Option(\"/Users/jenny/Desktop/output\")\n",
    "        case \"stage\" => Option(\"/Users/jenny/Desktop/output\")\n",
    "        case \"prod\" => Option(\"/Users/jenny/Desktop/output\")\n",
    "        case _ => None\n",
    "          throw new Exception(s\"env error, env name can either be test, stage, prod \\ncannot be ${env()}\")\n",
    "      })\n",
    "\n",
    "  val datePrefix: ScallopOption[String] =\n",
    "    opt[String](\"date.prefix\",\n",
    "      descr = \"date prefix for path\",\n",
    "      required = false,\n",
    "      default = Option(\"start_date\")) //show environment handling\n",
    "\n",
    "  val processDate: ScallopOption[String] =\n",
    "    opt[String](\"process.date\",\n",
    "      descr = \"date to string\" +\n",
    "        \", in YYYY-MM-DD format\",\n",
    "      required = true)\n",
    "\n",
    "  val uniqueUserPath: ScallopOption[String] =\n",
    "    opt[String](\"unique.user.path\",\n",
    "      descr = \"path to save unique user id and start date info\",\n",
    "      required = false,\n",
    "      default = Option(\"/Users/jenny/Desktop/unique-user\"))\n",
    "\n",
    "  val dayAgo: ScallopOption[Int] =\n",
    "    opt[Int](\"day.ago\",\n",
    "      descr = \"how many day ago you are going to overwrite\",\n",
    "      required = false,\n",
    "      default = Option(1))\n",
    "\n",
    "  verify()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Utils\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// choose the columns \n",
    "import org.apache.spark.sql.DataFrame\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import java.io.File\n",
    "\n",
    "import scala.collection.JavaConversions._\n",
    "import org.apache.spark.internal.Logging\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.joda.time.DateTime\n",
    "import org.joda.time.format.{DateTimeFormat, DateTimeFormatter}\n",
    "\n",
    "object Utils extends Logging {\n",
    "\n",
    "  def selectColumns(conf: CohortConf, sourceKey: String, inputDf: DataFrame): DataFrame = {\n",
    "    val fields          = getListFromConf(conf.selectColumnsConfigFile(), sourceKey).map(col)\n",
    "    val outputDf        = inputDf.select(fields: _*)\n",
    "    outputDf\n",
    "  }\n",
    "\n",
    "  def getListFromConf(configFileName: String, confKey: String): List[String] = {\n",
    "    try {\n",
    "      ConfigFactory.load(ConfigFactory.parseFile(new File(configFileName))).getStringList(confKey).toList\n",
    "      //ConfigFactory.load(configFileName).getStringList(confKey).toList\n",
    "    } catch {\n",
    "      case e: Exception =>\n",
    "        logError(s\"*** Error parsing for $confKey as List[String] from $configFileName ***\\n${e.getMessage}\")\n",
    "        List[String]()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def pathGenerator(inputParentPath: String, datePrefix: String, processDate: String): String = {\n",
    "    s\"$inputParentPath/$datePrefix=$processDate/\"\n",
    "  }\n",
    "\n",
    "  def dayAgoDateString(conf: CohortConf, dayAgo: Int): String = {\n",
    "    val dateFormat: DateTimeFormatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "    val processDate: DateTime         = DateTime.parse(conf.processDate(), dateFormat)\n",
    "    dateFormat.print(processDate.minusDays(dayAgo))\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait BikeShareTripReader\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.internal.Logging\n",
    "import org.apache.spark.sql.functions.{col, lit}\n",
    "import org.apache.spark.sql.types.{DoubleType, StringType}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.joda.time.DateTime\n",
    "import org.joda.time.format.{DateTimeFormat, DateTimeFormatter}\n",
    "\n",
    "trait BikeShareTripReader extends Logging{\n",
    "\n",
    "  def readBikeShareTrip(conf: CohortConf, spark: SparkSession): DataFrame = {\n",
    "    val path = Utils.pathGenerator(conf.inputBikeSharePath(), conf.datePrefix(), conf.processDate())\n",
    "\n",
    "logInfo(\"reading from %s\".format(path))\n",
    "\n",
    "val bikeShareDf: DataFrame = try {\n",
    "Some(spark.read.json(path)).get\n",
    "} catch {\n",
    "case e: Exception => spark.emptyDataFrame\n",
    ".withColumn(\"user_id\", lit(null: StringType))\n",
    ".withColumn(\"subscriber_type\", lit(null: StringType))\n",
    ".withColumn(\"start_station_id\", lit(null: StringType))\n",
    ".withColumn(\"end_station_id\", lit(null: StringType))\n",
    ".withColumn(\"zip_code\", lit(null: StringType))\n",
    ".withColumn(\"duration_sec\", lit(null: DoubleType))\n",
    ".withColumn(\"start_timestamp\", lit(null: StringType))\n",
    "}\n",
    "Utils.selectColumns(conf, \"bike.share.trip\", bikeShareDf)\n",
    "}\n",
    "\n",
    "def readDayAgoBikeShareTrip(conf: CohortConf, spark: SparkSession): DataFrame = {\n",
    "val path = dayAgoReadDataOutPath(conf)\n",
    "\n",
    "logInfo(\"reading from %s\".format(path))\n",
    "\n",
    "val bikeShareDf: DataFrame = try {\n",
    "Some(spark.read.json(path)).get\n",
    "} catch {\n",
    "case e: Exception => spark.emptyDataFrame\n",
    ".withColumn(\"user_id\", lit(null: StringType))\n",
    ".withColumn(\"subscriber_type\", lit(null: StringType))\n",
    ".withColumn(\"start_station_id\", lit(null: StringType))\n",
    ".withColumn(\"end_station_id\", lit(null: StringType))\n",
    ".withColumn(\"zip_code\", lit(null: StringType))\n",
    ".withColumn(\"avg_duration_sec\", lit(null: DoubleType))\n",
    "}\n",
    "    bikeShareDf\n",
    "  }\n",
    "\n",
    "  def dayAgoReadDataOutPath(conf: CohortConf): String = {\n",
    "    val dateString = Utils.dayAgoDateString(conf, conf.dayAgo())\n",
    "\n",
    "    val path : String = conf.dayAgo() match {\n",
    "      case 1 => Utils.pathGenerator(conf.outputDataPath(), conf.datePrefix(), dateString)\n",
    "      case 3 => Utils.pathGenerator(conf.outputDataPath()+\"/1\", conf.datePrefix(), dateString)\n",
    "      case 7 => Utils.pathGenerator(conf.outputDataPath()+\"/3\", conf.datePrefix(), dateString)\n",
    "      case _ => throw new Exception(\"input date is invalid\")\n",
    "    }\n",
    "    path\n",
    "  }\n",
    "\n",
    "  def dayAgoWriteDataOutPath(conf: CohortConf): String = {\n",
    "    val dateString = Utils.dayAgoDateString(conf, conf.dayAgo())\n",
    "\n",
    "    val path : String = conf.dayAgo() match {\n",
    "      case 1 => Utils.pathGenerator(conf.outputDataPath()+\"/1\", conf.datePrefix(), dateString)\n",
    "      case 3 => Utils.pathGenerator(conf.outputDataPath()+\"/3\", conf.datePrefix(), dateString)\n",
    "      case 7 => Utils.pathGenerator(conf.outputDataPath()+\"/7\", conf.datePrefix(), dateString)\n",
    "      case _ => throw new Exception(\"input date is invalid\")\n",
    "    }\n",
    "    path\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait BikeStationInfoReader\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.internal.Logging\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "trait BikeStationInfoReader extends Logging{\n",
    "\n",
    "  def readBikeStation(conf: CohortConf, spark: SparkSession): DataFrame = {\n",
    "    val path = \"%s/bike-station-info\".format(conf.inputMetaDataPath())\n",
    "\n",
    "    logInfo(\"reading from %s\".format(path))\n",
    "\n",
    "    val bikeStationDf = spark.read.json(path)\n",
    "    Utils.selectColumns(conf, \"bike.station.info\", bikeStationDf)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait UserReader\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.internal.Logging\n",
    "import org.apache.spark.sql.functions.lit\n",
    "import org.apache.spark.sql.types.StringType\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "trait UserReader extends Logging{\n",
    "\n",
    "  def readUserInfo(conf: CohortConf, spark: SparkSession, date: String): DataFrame = {\n",
    "    val inputPath = Utils.pathGenerator(conf.uniqueUserPath(), conf.datePrefix(), date)\n",
    "\n",
    "    logInfo(\"reading from %s\".format(inputPath))\n",
    "\n",
    "    val inputUniqueUsersDf: DataFrame = try { //reading unique user list\n",
    "      Some(spark.read.json(inputPath)).get\n",
    "    } catch {\n",
    "      case e: Exception => spark.emptyDataFrame.withColumn(\"user_id\", lit(null: StringType))\n",
    "        .withColumn(\"first_timestamp\", lit(null: StringType))\n",
    "    }\n",
    "\n",
    "    inputUniqueUsersDf\n",
    "  }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object UserProcess\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.internal.Logging\n",
    "import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "object UserProcess extends Logging with UserReader with BikeShareTripReader {\n",
    "\n",
    "  val spark = SparkSession\n",
    "    .builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Unique-users\")\n",
    "    .getOrCreate()\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = new CohortConf(args)\n",
    "\n",
    "    val inputPath = Utils.pathGenerator(conf.inputBikeSharePath(), conf.datePrefix(), conf.processDate())\n",
    "\n",
    "    val outputUniqueUser = Utils.pathGenerator(conf.uniqueUserPath(), conf.datePrefix(), conf.processDate())\n",
    "\n",
    "    uniqueUser(outputUniqueUser, inputPath, conf)\n",
    "  }\n",
    "\n",
    "  def uniqueUser(uniqueUsersPath: String, inputBikeSharePath: String, conf: CohortConf): Unit = {\n",
    "\n",
    "    val inputUniqueUsersDf = readUserInfo(conf, spark, Utils.dayAgoDateString(conf, 1))\n",
    "    val inputBikeShareDf = readBikeShareTrip(conf, spark)\n",
    "\n",
    "    val users = Utils.selectColumns(conf, \"bike.unique.user\", inputBikeShareDf)\n",
    "      .withColumn(\"first_timestamp\", col(\"start_timestamp\"))\n",
    "      .drop(col(\"start_timestamp\"))\n",
    "\n",
    "    val uniqueUserDf = inputUniqueUsersDf.unionByName(users)\n",
    "      .groupBy(\"user_id\")\n",
    "      .agg(min(\"first_timestamp\").as(\"first_timestamp\"))\n",
    "\n",
    "    uniqueUserDf.distinct().coalesce(1).write.mode(SaveMode.Overwrite).json(uniqueUsersPath)\n",
    "  }\n",
    "\n",
    "}\n",
    "UserProcess.main(Array(\"--process.date\",\"2014-01-01\",\"--select.columns.config\",\"/Users/jenny/Desktop/conf/select-columns.conf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object BikeShareProcess\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.internal.Logging\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\n",
    "\n",
    "object BikeShareProcess extends Logging with BikeShareTripReader {\n",
    "\n",
    "  val fields = List(\"user_id\",\n",
    "    \"subscriber_type\",\n",
    "    \"start_station_id\",\n",
    "    \"end_station_id\",\n",
    "    \"zip_code\")\n",
    "\n",
    "  val avgDurationSec = \"avg_duration_sec\"\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = new CohortConf(args)\n",
    "    val spark = SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .appName(\"Bike-share\")\n",
    "      .getOrCreate()\n",
    "\n",
    "    val outputPath = Utils.pathGenerator(conf.outputDataPath(), conf.datePrefix(), conf.processDate())\n",
    "\n",
    "    bikeShareAgg(spark, conf, outputPath)\n",
    "  }\n",
    "\n",
    "  def bikeShareAgg(spark: SparkSession, conf: CohortConf, outputPath: String): Unit = {\n",
    "\n",
    "    val bikeShareDf = readBikeShareTrip(conf, spark)\n",
    "\n",
    "    val bikeShareAggDf = bikeShareDf\n",
    "      .groupBy(fields.map(col):_*)\n",
    "      .agg(avg(col(\"duration_sec\")).as(avgDurationSec))\n",
    "\n",
    "    bikeShareAggDf.coalesce(1).write.mode(SaveMode.Overwrite).json(outputPath)\n",
    "  }\n",
    "\n",
    "}\n",
    "BikeShareProcess.main(Array(\"--process.date\",\"2014-01-01\",\"--select.columns.config\",\"/Users/jenny/Desktop/conf/select-columns.conf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object RetentionProcess\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.internal.Logging\n",
    "import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "object RetentionProcess extends Logging with UserReader with BikeShareTripReader {\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf                  = new CohortConf(args)\n",
    "    val spark = SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .appName(\"Bike-share\")\n",
    "      .getOrCreate()\n",
    "    spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "    retentionPrep(spark, conf)\n",
    "  }\n",
    "\n",
    "  def retentionPrep(spark: SparkSession, conf: CohortConf): Unit = {\n",
    "    val bikeShareDf = readBikeShareTrip(conf, spark)\n",
    "    bikeShareDf.printSchema()\n",
    "    val userDf = readUserInfo(conf, spark, conf.processDate())\n",
    "    userDf.printSchema()\n",
    "    val dayAgoBikeShareDf = readDayAgoBikeShareTrip(conf, spark)\n",
    "    dayAgoBikeShareDf.printSchema()\n",
    "\n",
    "    val joinedBikeSharedDf = bikeShareDf.join(userDf,\n",
    "      bikeShareDf.col(\"user_id\") === userDf.col(\"user_id\"), \"left\")\n",
    "      .drop(bikeShareDf.col(\"user_id\"))\n",
    "\n",
    "    val bikeUserAgeDays = joinedBikeSharedDf\n",
    "      .withColumn(\"user_age_days\",\n",
    "        datediff(to_date(col(\"start_timestamp\")), to_date(col(\"first_timestamp\"))))\n",
    "\n",
    "    val bikeFilteredDf : DataFrame = conf.dayAgo() match {\n",
    "      case 1 => bikeUserAgeDays.filter((col(\"user_age_days\") === 1))\n",
    "      case 3 => bikeUserAgeDays.filter((col(\"user_age_days\") === 3))\n",
    "      case 7 => bikeUserAgeDays.filter((col(\"user_age_days\") === 7))\n",
    "      case _ => throw new Exception(\"input date is invalid\")\n",
    "    }\n",
    "\n",
    "    val bikeFilteredAgoDf = bikeFilteredDf.select(\"user_id\", \"user_age_days\").distinct()\n",
    "\n",
    "    val aggPrepDf = dayAgoBikeShareDf\n",
    "      .join(bikeFilteredAgoDf, dayAgoBikeShareDf.col(\"user_id\") === bikeFilteredAgoDf.col(\"user_id\"), \"left\")\n",
    "      .drop(bikeFilteredAgoDf.col(\"user_id\"))\n",
    "\n",
    "    val groupbyFields = BikeShareProcess.fields :+ BikeShareProcess.avgDurationSec\n",
    "\n",
    "    if(!aggPrepDf.columns.contains(\"user_age_days\") || aggPrepDf.count() == 0){\n",
    "      logInfo(\"didn't find anyone fit into %s day ago\".format(conf.dayAgo()))\n",
    "      val aggPrepDfWithageDays = aggPrepDf.withColumn(\"user_age_days\", lit(0))\n",
    "      retentionAndSave(aggPrepDfWithageDays, conf)\n",
    "    } else {\n",
    "      retentionAndSave(aggPrepDf, conf)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def retentionAndSave(df: DataFrame, conf: CohortConf): Unit = {\n",
    "    val groupbyFields = BikeShareProcess.fields :+ BikeShareProcess.avgDurationSec :+ \"user_age_days\"\n",
    "\n",
    "    val bikeUserAggDf = df.groupBy(groupbyFields.map(col):_*)\n",
    "      .agg(max(when(df.col(\"user_age_days\") === 1, 1).otherwise(0)).alias(\"retention_1\"),\n",
    "        max(when(df.col(\"user_age_days\") === 3, 1).otherwise(0)).alias(\"retention_3\"),\n",
    "        max(when(df.col(\"user_age_days\") === 7, 1).otherwise(0)).alias(\"retention_7\"))\n",
    "\n",
    "    val outputPath = dayAgoWriteDataOutPath(conf)\n",
    "\n",
    "    bikeUserAggDf.coalesce(1).write.mode(SaveMode.Overwrite).json(outputPath)\n",
    "  }\n",
    "}\n",
    "RetentionProcess.main(Array(\"--process.date\",\"2014-01-01\",\"--select.columns.config\",\"/Users/jenny/Desktop/conf/select-columns.conf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.twitter4j:twitter4j-core:4.0.4 for download\n",
      "Obtained 2 files\n",
      "Marking org.apache.bahir:spark-streaming-twitter_2.11:2.3.3 for download\n",
      "Obtained 2 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Magic AddDeps failed to execute with error: \n",
       "requirement failed: Can only call getServletHandlers on a running MetricsSystem"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%AddDeps org.twitter4j twitter4j-core 4.0.4\n",
    "%AddDeps org.apache.bahir spark-streaming-twitter_2.11 2.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Utils\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import twitter4j.Status\n",
    "import scala.io.{AnsiColor, Source}\n",
    "\n",
    "object Utils {\n",
    "\n",
    "  // Some type aliases to give a little bit of context\n",
    "  type Tweet = Status\n",
    "  type TweetText = String\n",
    "  type Sentence = Seq[String]\n",
    "\n",
    "  private def format(n: Int): String = f\"$n%2d\"\n",
    "\n",
    "  private def wrapScore(s: String): String = s\"[ $s ] \"\n",
    "\n",
    "  private def makeReadable(n: Int): String =\n",
    "    if (n > 0)      s\"${AnsiColor.GREEN + format(n) + AnsiColor.RESET}\"\n",
    "    else if (n < 0) s\"${AnsiColor.RED   + format(n) + AnsiColor.RESET}\"\n",
    "    else            s\"${format(n)}\"\n",
    "\n",
    "  private def makeReadable(s: String): String =\n",
    "    s.takeWhile(_ != '\\n').take(80) + \"...\" //short it ?\n",
    "\n",
    "  def makeReadable(sn: (String, Int)): String =\n",
    "    sn match {\n",
    "      case (tweetText, score) => s\"${wrapScore(makeReadable(score))}${makeReadable(tweetText)}\"\n",
    "    }\n",
    "\n",
    "  def load(resourcePath: String): Set[String] = {\n",
    "    //val source = Source.fromInputStream(getClass.getResourceAsStream(resourcePath))\n",
    "    //val words = source.getLines.toSet\n",
    "    //source.close()\n",
    "    //words\n",
    "    Source.fromFile(resourcePath).getLines.toSet\n",
    "  }\n",
    "\n",
    "  def wordsOf(tweet: TweetText): Sentence =\n",
    "    tweet.split(\" \")\n",
    "\n",
    "  def toLowercase(sentence: Sentence): Sentence =\n",
    "    sentence.map(_.toLowerCase)\n",
    "\n",
    "  def keepActualWords(sentence: Sentence): Sentence =\n",
    "    sentence.filter(_.matches(\"[a-z]+\"))\n",
    "\n",
    "  def extractWords(sentence: Sentence): Sentence =\n",
    "    sentence.map(_.toLowerCase).filter(_.matches(\"[a-z]+\"))\n",
    "\n",
    "  def keepMeaningfulWords(sentence: Sentence, uselessWords: Set[String]): Sentence =\n",
    "    sentence.filterNot(word => uselessWords.contains(word))\n",
    "\n",
    "  def computeScore(words: Sentence, positiveWords: Set[String], negativeWords: Set[String]): Int =\n",
    "    words.map(word => computeWordScore(word, positiveWords, negativeWords)).sum\n",
    "\n",
    "  def computeWordScore(word: String, positiveWords: Set[String], negativeWords: Set[String]): Int =\n",
    "    if (positiveWords.contains(word)) 1\n",
    "    else if (negativeWords.contains(word)) -1\n",
    "    else 0\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object TwitterSentimentScore\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.dstream.DStream\n",
    "import org.apache.spark.streaming.twitter.TwitterUtils\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import twitter4j.Status\n",
    "\n",
    "object TwitterSentimentScore extends App {\n",
    "\n",
    "  // You can find all functions used to process the stream in the\n",
    "  // Utils.scala source file, whose contents we import here\n",
    "  import Utils._\n",
    "\n",
    "  // First, let's configure Spark\n",
    "  // We have to at least set an application name and master\n",
    "  // If no master is given as part of the configuration we\n",
    "  // will set it to be a local deployment running an\n",
    "  // executor per thread\n",
    "  val sparkConfiguration = new SparkConf().\n",
    "    setAppName(\"spark-twitter-stream-example\").\n",
    "    setMaster(sys.env.get(\"spark.master\").getOrElse(\"local[*]\"))\n",
    "    \n",
    "  // Let's create the Spark Context using the configuration we just created\n",
    "  val sparkContext = new SparkContext(sparkConfiguration)\n",
    "\n",
    "  // Now let's wrap the context in a streaming one, passing along the window size\n",
    "  val streamingContext = new StreamingContext(sparkContext, Seconds(10))\n",
    "\n",
    "  // Creating a stream from Twitter, filter out english only\n",
    "  val tweets: DStream[Status] =\n",
    "    TwitterUtils.createStream(streamingContext, None).filter(_.getLang == \"en\")\n",
    "\n",
    "  // To compute the sentiment of a tweet we'll use different set of words used to\n",
    "  // filter and score each word of a sentence. Since these lists are pretty small\n",
    "  // it can be worthwhile to broadcast those across the cluster so that every\n",
    "  // executor can access them locally\n",
    "  val uselessWords = sparkContext.broadcast(load(\"/Users/jenny/Desktop/stream/stop-words.dat\"))\n",
    "  val positiveWords = sparkContext.broadcast(load(\"/Users/jenny/Desktop/stream/pos-words.dat\"))\n",
    "  val negativeWords = sparkContext.broadcast(load(\"/Users/jenny/Desktop/stream/neg-words.dat\"))\n",
    "\n",
    "  // Let's extract the words of each tweet\n",
    "  // We'll carry the tweet along in order to print it in the end\n",
    "  val textAndSentences: DStream[(TweetText, Sentence)] =\n",
    "  tweets.\n",
    "    map(_.getText).\n",
    "    map(tweetText => (tweetText, wordsOf(tweetText)))\n",
    "\n",
    "  // Apply several transformations that allow us to keep just meaningful sentences\n",
    "  val textAndMeaningfulSentences: DStream[(TweetText, Sentence)] =\n",
    "    textAndSentences.\n",
    "      mapValues(toLowercase).\n",
    "      mapValues(keepActualWords).\n",
    "      mapValues(words => keepMeaningfulWords(words, uselessWords.value)).\n",
    "      filter { case (_, sentence) => sentence.length > 0 }\n",
    "\n",
    "  // Compute the score of each sentence and keep only the non-neutral ones\n",
    "  val textAndNonNeutralScore: DStream[(TweetText, Int)] =\n",
    "    textAndMeaningfulSentences.\n",
    "      mapValues(sentence => computeScore(sentence, positiveWords.value, negativeWords.value)).\n",
    "      filter { case (_, score) => score != 0 }\n",
    "\n",
    "  // Transform the (tweet, score) pair into a readable string and print it\n",
    "  textAndNonNeutralScore.map(makeReadable).print\n",
    "  textAndNonNeutralScore.saveAsTextFiles(\"tweets\", \"json\")\n",
    "\n",
    "  // Now that the streaming is defined, start it\n",
    "  streamingContext.start()\n",
    "\n",
    "  // Let's await the stream to end - forever\n",
    "  streamingContext.awaitTermination()\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark2.4.5 - Scala",
   "language": "scala",
   "name": "spark2.4.5_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
